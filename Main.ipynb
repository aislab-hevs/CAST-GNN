{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59ee0d75",
   "metadata": {},
   "source": [
    "This notebook is the single source for all datasets.\n",
    "Choose your dataset in the next cell. For **Something-Something** we use\n",
    "`chunk_size=100`, `meta_lr=0.01`, and `ReduceLROnPlateau`.\n",
    "For **HMDB/Kinetics/UCF** we use `chunk_size=300`, `meta_lr=0.001`, and `StepLR`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15310962-3c32-4b38-90f5-81f8a56a0b9f",
   "metadata": {},
   "source": [
    "Import require libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690948d6-e7fa-4943-a40b-49fb97c4d405",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, TransformerConv\n",
    "from torch_geometric.data import Data\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import copy  # for KD teacher\n",
    "\n",
    "try:\n",
    "    import psutil\n",
    "except ImportError:\n",
    "    psutil = None\n",
    "\n",
    "try:\n",
    "    np.float\n",
    "except AttributeError:\n",
    "    np.float = float\n",
    "    np.int = int\n",
    "    np.bool = bool\n",
    "    np.object = object\n",
    "    np.str = str\n",
    "    np.long = int\n",
    "\n",
    "from river.drift import ADWIN, PageHinkley\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "try:\n",
    "    from sklearn_extra.cluster import KMedoids\n",
    "except ImportError:\n",
    "    KMedoids = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91645fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose one: \"HMDB\", \"Kinetics\", \"UCF\", \"Something-Something\"\n",
    "DATASET = \"UCF\"  # change name of the dataset \n",
    "\n",
    "if DATASET == \"Something-Something\":\n",
    "    CHUNK_SIZE = 100\n",
    "    META_LR    = 1e-2\n",
    "    SCHEDULER  = \"plateau\"   # ReduceLROnPlateau (uses val F1)\n",
    "else:\n",
    "    CHUNK_SIZE = 300\n",
    "    META_LR    = 1e-3\n",
    "    SCHEDULER  = \"step\"      # StepLR\n",
    "\n",
    "print(f\"Dataset={DATASET} | chunk={CHUNK_SIZE} | meta_lr={META_LR} | scheduler={SCHEDULER}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdfa93a-7a41-4768-a278-e492f0aead16",
   "metadata": {},
   "source": [
    "Memory and latency helpers functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d14677-5b34-49a5-bd13-f43eb7771839",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_usage():\n",
    "    if psutil is not None:\n",
    "        process = psutil.Process(os.getpid())\n",
    "        return process.memory_info().rss / (1024.0 * 1024.0)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_cuda_memory_usage():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "        return torch.cuda.memory_allocated() / (1024.0 * 1024.0)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def plot_memory_usage(cpu_mem, gpu_mem=None):\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.plot(cpu_mem, marker='o', label=\"CPU RAM (MB)\")\n",
    "    if gpu_mem is not None:\n",
    "        plt.plot(gpu_mem, marker='s', label=\"GPU VRAM (MB)\")\n",
    "    plt.title(\"Memory Usage per Test Chunk\")\n",
    "    plt.xlabel(\"Test Chunk\")\n",
    "    plt.ylabel(\"Memory (MB)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c9d73f-4b84-486a-8315-f04f97bb6184",
   "metadata": {},
   "source": [
    "Load STKG data from your local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebf590d-b863-4b47-b7c7-72914bd25566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_files(): # give full path names of files in de Data folder\n",
    "    node_features = []\n",
    "    with open('.../node_features.txt','r') as f:\n",
    "        for line in f:\n",
    "            node_features.append([float(x) for x in line.strip().split()[1:]])\n",
    "    x = torch.tensor(node_features, dtype=torch.float)\n",
    "\n",
    "    edge_index = []\n",
    "    with open('.../edges.txt','r') as f:\n",
    "        for line in f:\n",
    "            edge_index.append([int(x) for x in line.strip().split()])\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "\n",
    "    edge_attr = []\n",
    "    with open('.../edge_features.txt','r') as f:\n",
    "        for line in f:\n",
    "            edge_attr.append([float(x) for x in line.strip().split()[2:]])\n",
    "    edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "\n",
    "    y = []\n",
    "    with open('.../node_labels.txt','r') as f:\n",
    "        for line in f:\n",
    "            y.append(int(line.strip().split()[1]))\n",
    "    y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    return Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b70da0-6819-4598-86ec-596026875d34",
   "metadata": {},
   "source": [
    "The definition of continual learning metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120f5052-b767-4e94-882a-93a35cf39d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLMetrics:\n",
    "    def __init__(self, num_chunks):\n",
    "        self.best_acc_per_chunk = [0. for _ in range(num_chunks)]\n",
    "        self.last_acc_per_chunk = [0. for _ in range(num_chunks)]\n",
    "        self.first_acc_per_chunk = [0. for _ in range(num_chunks)]\n",
    "        self.seen = [False for _ in range(num_chunks)]\n",
    "        self.chunk_history = [[] for _ in range(num_chunks)]\n",
    "# Adaptation epochs with highest accuracy\n",
    "        self.adaptation_epochs = [None for _ in range(num_chunks)]\n",
    "\n",
    "    def update(self, chunk_idx, acc, epoch):\n",
    "        self.chunk_history[chunk_idx].append(acc)\n",
    "        if acc > self.best_acc_per_chunk[chunk_idx]:\n",
    "            self.best_acc_per_chunk[chunk_idx] = acc\n",
    "            self.adaptation_epochs[chunk_idx] = epoch\n",
    "        if not self.seen[chunk_idx]:\n",
    "            self.first_acc_per_chunk[chunk_idx] = acc\n",
    "            self.seen[chunk_idx] = True\n",
    "        self.last_acc_per_chunk[chunk_idx] = acc\n",
    "\n",
    "    def average_forgetting(self): # Average of each chunk (best - last)\n",
    "        diffs = [b - l for b, l in zip(self.best_acc_per_chunk, self.last_acc_per_chunk)]\n",
    "        return np.mean(diffs)\n",
    "\n",
    "    def average_forgetting_std(self):\n",
    "        diffs = [b - l for b, l in zip(self.best_acc_per_chunk, self.last_acc_per_chunk)]\n",
    "        return np.std(diffs)\n",
    "\n",
    "    def adaptation_speed(self):\n",
    "        valid = [e for e in self.adaptation_epochs if e is not None]\n",
    "        return np.mean(valid) if valid else None\n",
    "\n",
    "    def adaptation_speed_std(self):\n",
    "        valid = [e for e in self.adaptation_epochs if e is not None]\n",
    "        return np.std(valid) if valid else None\n",
    "\n",
    "    def report(self):\n",
    "        print(\"\\n=== Continual Learning Metrics ===\")\n",
    "        avg_forget = self.average_forgetting()\n",
    "        std_forget = self.average_forgetting_std()\n",
    "        avg_adapt = self.adaptation_speed()\n",
    "        std_adapt = self.adaptation_speed_std()\n",
    "        print(f\" Average Forgetting: {avg_forget:.4f} ± {std_forget:.4f}\")\n",
    "        if avg_adapt is not None:\n",
    "            print(f\" Adaptation Speed (epoch of peak acc): {avg_adapt:.4f} ± {std_adapt:.4f}\")\n",
    "        else:\n",
    "            print(f\" Adaptation Speed (epoch of peak acc): None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed4ce60-9e9e-4703-bf9b-aecb05477838",
   "metadata": {},
   "source": [
    "Definition of drift function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12951a6c-3d3b-4665-8e85-d204b98353a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_concept_drift(acc_history, window=5, threshold=2.5):\n",
    "    drifts = []\n",
    "    for i in range(len(acc_history)):\n",
    "        if i < window:\n",
    "            drifts.append(False)\n",
    "        else:\n",
    "            window_mean = np.mean(acc_history[i - window:i])\n",
    "            window_std = np.std(acc_history[i - window:i]) + 1e-8\n",
    "            if acc_history[i] < window_mean - threshold * window_std:\n",
    "                drifts.append(True)\n",
    "            else:\n",
    "                drifts.append(False)\n",
    "    return drifts\n",
    "\n",
    "def detect_drift_adwin(acc_history, delta=0.1):\n",
    "    adwin = ADWIN(delta=delta)\n",
    "    drifts = []\n",
    "    for acc in acc_history:\n",
    "        adwin.update(acc)\n",
    "        drifts.append(adwin.drift_detected)\n",
    "    return drifts\n",
    "\n",
    "def detect_drift_pagehinkley(acc_history, threshold=50, alpha=0.999, min_instances=5):\n",
    "    ph = PageHinkley(threshold=threshold, alpha=alpha, min_instances=min_instances)\n",
    "    drifts = []\n",
    "    for acc in acc_history:\n",
    "        ph.update(acc)\n",
    "        drifts.append(ph.drift_detected)\n",
    "    return drifts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88835d1a-faa2-44b6-ae41-1924687ed5b5",
   "metadata": {},
   "source": [
    "Definition of forgetting rate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe47f54-a75b-4cae-9439-cf99bf261719",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_acc_history = defaultdict(list)\n",
    "def windowed_forgetting_rate(chunk_id, current_acc, window_size=5):\n",
    "    history = chunk_acc_history[chunk_id]\n",
    "    if not history:\n",
    "        chunk_acc_history[chunk_id].append(current_acc)\n",
    "        return 0.0\n",
    "    window = history[-window_size:]\n",
    "    forget = sum(max(0, prev - current_acc) for prev in window) / len(window)\n",
    "    chunk_acc_history[chunk_id].append(current_acc)\n",
    "    return forget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8d705c-a786-4962-8765-441b527f7a0c",
   "metadata": {},
   "source": [
    "Definition of plots for drifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290af996-0ebe-4a7b-b7c8-1d3c7b558929",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_adwin_drift(accs, drift_flags):\n",
    "    x = np.arange(len(accs))\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.plot(x, accs, label='Chunk Accuracy')\n",
    "    drift_points = [i for i, flag in enumerate(drift_flags) if flag]\n",
    "    plt.scatter(drift_points, [accs[i] for i in drift_points], color='red', label='ADWIN Drift Detected', zorder=5)\n",
    "    plt.xlabel(\"Test Chunk\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"ADWIN Concept Drift Detection\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_drift_detection(values, drift_flags, metric_name=\"Accuracy\"):\n",
    "    x = np.arange(len(values))\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.plot(x, values, label=f\"Chunk {metric_name}\")\n",
    "    drift_points = [i for i, flag in enumerate(drift_flags) if flag]\n",
    "    plt.scatter(drift_points, [values[i] for i in drift_points], color='red', label='Drift Detected', zorder=5)\n",
    "    plt.xlabel(\"Test Chunk\")\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.title(f\"Z-Score Concept Drift Detection ({metric_name})\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a415a6b8-e69c-46c2-b466-ebbc7ed7dd4c",
   "metadata": {},
   "source": [
    "Focal loss definition handling class imbalance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a675f94-4a90-44dd-8100-c2f9cfcffd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ClasswiseFocalLoss(torch.nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=None, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        CE_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-CE_loss)\n",
    "        gamma_t = self.gamma[targets] if self.gamma is not None else 2.0\n",
    "        alpha_t = self.alpha[targets] if self.alpha is not None else 1.0\n",
    "        focal_loss = alpha_t * (1 - pt) ** gamma_t * CE_loss\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "def get_classwise_focal_loss(num_classes, device):\n",
    "    gamma_per_class = torch.tensor([2.0] * num_classes, dtype=torch.float).to(device)\n",
    "    alpha_per_class = torch.tensor([1.0] * num_classes, dtype=torch.float).to(device)\n",
    "    return ClasswiseFocalLoss(alpha=alpha_per_class, gamma=gamma_per_class)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7054a0ce-d8bc-47f1-9736-d439f207cc64",
   "metadata": {},
   "source": [
    "Definition of confusion matrix before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7ddc9a-d27f-4899-9ec3-d5d2938dd550",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, num_classes):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(num_classes)))\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=list(range(num_classes)),\n",
    "                yticklabels=list(range(num_classes)))\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39d29e1-af26-43d6-91b0-e9eb098477b8",
   "metadata": {},
   "source": [
    "Preparing data for continuous learning real time simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668eb9a8-cc84-47c2-a4e6-86f506b948fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_and_split_data_into_time_chunks(data, timestamps, chunk_size=500, overlap_ratio=0.1):\n",
    "    overlap = int(chunk_size * overlap_ratio)\n",
    "    step_size = chunk_size - overlap\n",
    "    max_time = timestamps.max().item()\n",
    "    num_chunks = int((max_time - chunk_size) // step_size) + 1\n",
    "    chunks = []\n",
    "    for i in range(num_chunks):\n",
    "        start_time = i * step_size\n",
    "        end_time = start_time + chunk_size\n",
    "        node_mask = (timestamps >= start_time) & (timestamps < end_time)\n",
    "        selected_nodes = torch.nonzero(node_mask, as_tuple=False).squeeze()\n",
    "        if selected_nodes.numel() == 0:\n",
    "            continue\n",
    "        if selected_nodes.dim() == 0:\n",
    "            selected_nodes = selected_nodes.unsqueeze(0)\n",
    "        selected_nodes = selected_nodes.tolist()\n",
    "\n",
    "        node_id_map = {old_id: new_id for new_id, old_id in enumerate(selected_nodes)}\n",
    "        chunk_x = data.x[selected_nodes]\n",
    "        chunk_y = data.y[selected_nodes]\n",
    "\n",
    "        edge_mask = [(src in selected_nodes and dst in selected_nodes)\n",
    "                     for src, dst in data.edge_index.t().tolist()]\n",
    "        edge_mask = torch.tensor(edge_mask, dtype=torch.bool)\n",
    "        chunk_edge_index = data.edge_index[:, edge_mask]\n",
    "\n",
    "        chunk_edge_attr = None\n",
    "        if data.edge_attr is not None and data.edge_attr.size(0) == data.edge_index.size(1):\n",
    "            chunk_edge_attr = data.edge_attr[edge_mask]\n",
    "\n",
    "        if chunk_edge_index.numel() > 0:\n",
    "            reindexed_edges = [[node_id_map[e[0]], node_id_map[e[1]]] \n",
    "                               for e in chunk_edge_index.t().tolist()]\n",
    "            chunk_edge_index = torch.tensor(reindexed_edges).t().long()\n",
    "        else:\n",
    "            chunk_edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "\n",
    "        chunk_data = Data(x=chunk_x, y=chunk_y, edge_index=chunk_edge_index)\n",
    "        if chunk_edge_attr is not None:\n",
    "            chunk_data.edge_attr = chunk_edge_attr.clone().detach()\n",
    "\n",
    "        chunks.append(chunk_data)\n",
    "        print(f\"Chunk {i} created ({start_time}-{end_time}) with {len(selected_nodes)} nodes.\")\n",
    "    return chunks\n",
    "\n",
    "def split_chunks_time_aware(chunks, timestamps, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1):\n",
    "    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-5\n",
    "    chunk_time_means = []\n",
    "    node_start = 0\n",
    "    for chunk in chunks:\n",
    "        num_nodes = chunk.x.shape[0]\n",
    "        chunk_timestamps = timestamps[node_start:node_start + num_nodes]\n",
    "        chunk_time_mean = torch.mean(chunk_timestamps.float()).item()\n",
    "        chunk_time_means.append((chunk_time_mean, chunk))\n",
    "        node_start += num_nodes\n",
    "    chunk_time_means.sort(key=lambda x: x[0])\n",
    "    sorted_chunks = [c for _, c in chunk_time_means]\n",
    "    total = len(sorted_chunks)\n",
    "    train_end = int(total * train_ratio)\n",
    "    val_end = int(total * (train_ratio + val_ratio))\n",
    "    train_chunks = sorted_chunks[:train_end]\n",
    "    val_chunks = sorted_chunks[train_end:val_end]\n",
    "    test_chunks = sorted_chunks[val_end:]\n",
    "    return train_chunks, val_chunks, test_chunks\n",
    "\n",
    "def generate_balanced_timestamps(y):\n",
    "    indices = torch.arange(len(y))\n",
    "    shuffled_indices = indices[torch.randperm(len(indices))]\n",
    "    timestamps = torch.zeros_like(y, dtype=torch.float)\n",
    "    timestamps[shuffled_indices] = torch.arange(len(y)).float()\n",
    "    return timestamps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b56b4c9-c1a2-442e-9110-f153a9ee0158",
   "metadata": {},
   "source": [
    "Function and class definition of cache cleaning during continuous learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bce208-fc38-408b-a3e7-2507c55bcee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def kmedoids_clean(embeddings, max_size=300):\n",
    "    if KMedoids is None:\n",
    "        return np.arange(len(embeddings))\n",
    "    N = len(embeddings)\n",
    "    if N <= max_size:\n",
    "        return np.arange(N)\n",
    "    k = max_size // 3\n",
    "    if k < 1:\n",
    "        k = 1\n",
    "    kmed = KMedoids(n_clusters=k, method='pam', metric='euclidean',\n",
    "                    init='k-medoids++', random_state=42)\n",
    "    kmed.fit(embeddings)\n",
    "    centers = kmed.medoid_indices_\n",
    "    return centers\n",
    "\n",
    "\n",
    "class TGNMemory:\n",
    "    def __init__(self, hidden_dim=128):\n",
    "        self.memory_dict = {}\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "    def initialize_node(self, node_id):\n",
    "        if node_id not in self.memory_dict:\n",
    "            self.memory_dict[node_id] = torch.zeros(self.hidden_dim) # Create a zero vector for each node\n",
    "\n",
    "    def get_memory(self, node_id):\n",
    "        return self.memory_dict.get(node_id, torch.zeros(self.hidden_dim))\n",
    "\n",
    "    def update_memory(self, node_ids, new_states): # After each chunk in the training cycle, we will take the embedding1 output and update the memory\n",
    "        for i, nid in enumerate(node_ids):\n",
    "            nid = int(nid.item())\n",
    "            self.memory_dict[nid] = new_states[i].detach().cpu().clone()\n",
    "\n",
    "    def clear_memory(self):\n",
    "        self.memory_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57539e6-4c0f-49b1-93d0-1739b7a2a924",
   "metadata": {},
   "source": [
    "HybridSelectiveReplayBuffer definition for catastrophic forgetting mitigation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b78130-add3-4767-b18d-124165a1fafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class HybridSelectiveReplayBuffer:\n",
    "    def __init__(self, max_size=500, sampling_mode='proportional'):\n",
    "        self.buffer = []\n",
    "        self.max_size = max_size\n",
    "        self.sampling_mode = sampling_mode\n",
    "\n",
    "    def add(self, chunk, model):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits, _ = model(chunk.x, chunk.edge_index, edge_attr=chunk.edge_attr)\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            uncertainty = torch.var(probs, dim=1).mean().item()\n",
    "            embedding = torch.mean(chunk.x, dim=0).cpu().numpy()\n",
    "            class_dist = torch.bincount(chunk.y, minlength=probs.size(1)).float()\n",
    "            dominant_class = torch.argmax(class_dist).item()\n",
    "        self.buffer.append((chunk.clone().detach(), uncertainty, embedding, dominant_class))\n",
    "        if len(self.buffer) > self.max_size:\n",
    "            self.buffer.pop(0)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        from collections import defaultdict\n",
    "        if len(self.buffer) == 0:\n",
    "            return []\n",
    "        class_map = defaultdict(list)\n",
    "        for idx, item in enumerate(self.buffer):\n",
    "            class_map[item[3]].append(idx)\n",
    "        total_len = len(self.buffer)\n",
    "        classes = list(class_map.keys())\n",
    "        proportions = {c: len(class_map[c]) / total_len for c in classes}\n",
    "        selected_idxs = []\n",
    "        for c in classes:\n",
    "            n = int(round(proportions[c] * batch_size))\n",
    "            selected_idxs.extend(class_map[c][:n])\n",
    "        if len(selected_idxs) < batch_size:\n",
    "            leftover = [i for i in range(len(self.buffer)) if i not in selected_idxs]\n",
    "            leftover_sorted = sorted(leftover, key=lambda i: self.buffer[i][1], reverse=True)\n",
    "            needed = batch_size - len(selected_idxs)\n",
    "            selected_idxs.extend(leftover_sorted[:needed])\n",
    "        return [self.buffer[i][0] for i in selected_idxs]\n",
    "\n",
    "    def clean(self, max_size=300):\n",
    "        old_len = len(self.buffer)\n",
    "        if old_len <= max_size:\n",
    "            return\n",
    "        embeddings = np.array([item[2] for item in self.buffer])\n",
    "        keep_indices = kmedoids_clean(embeddings, max_size=max_size)\n",
    "        self.buffer = [self.buffer[idx] for idx in keep_indices]\n",
    "        print(f\"Buffer cleaning with K-Medoids: {old_len} -> {len(self.buffer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7f0e87-9237-4e5f-ba07-d0099df3d379",
   "metadata": {},
   "source": [
    "Self-attention to maintain high-level semantic consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada8cfcf-2e00-434b-a9b5-1c94f9b364b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class STSelfAttention(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, time_embed_dim=32, original_edge_dim=0):\n",
    "        super().__init__()\n",
    "        self.time_encoder = torch.nn.Linear(1, time_embed_dim)\n",
    "        self.combined_edge_dim = time_embed_dim + original_edge_dim\n",
    "        self.attn = TransformerConv(\n",
    "            in_channels=hidden_channels,\n",
    "            out_channels=hidden_channels,\n",
    "            heads=4,\n",
    "            edge_dim=self.combined_edge_dim\n",
    "        )\n",
    "        self.norm = torch.nn.LayerNorm(hidden_channels * 4)\n",
    "        self.linear = torch.nn.Linear(hidden_channels * 4, hidden_channels)\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr=None, timestamps=None):\n",
    "        E = edge_index.size(1)\n",
    "        if E == 0:\n",
    "            return x\n",
    "        if timestamps is not None and E > 0:\n",
    "            time_diff = torch.abs(timestamps[edge_index[0]] - timestamps[edge_index[1]]).unsqueeze(1)\n",
    "            time_embed = self.time_encoder(time_diff)\n",
    "        else:\n",
    "            time_embed = None\n",
    "\n",
    "        if edge_attr is None:\n",
    "            combined_edge_attr = time_embed\n",
    "        else:\n",
    "            if time_embed is not None:\n",
    "                combined_edge_attr = torch.cat([edge_attr, time_embed], dim=1)\n",
    "            else:\n",
    "                combined_edge_attr = edge_attr\n",
    "\n",
    "        out = self.attn(x, edge_index, combined_edge_attr)\n",
    "        out = self.norm(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.linear(out)\n",
    "        return self.dropout(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c470409a-2018-46eb-931d-a7aad22a59c1",
   "metadata": {},
   "source": [
    "This layer integrates structural and temporal information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df47323-f562-4586-95e3-0398319d83c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TemporalEmbeddingLayer(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, time_embed_dim=32):\n",
    "        super().__init__()\n",
    "        self.gcn = GCNConv(in_channels - 1, hidden_channels) # time property in the last column\n",
    "        self.time_encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(1, time_embed_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.LayerNorm(time_embed_dim)\n",
    "        )\n",
    "        self.feature_fusion = torch.nn.Linear(hidden_channels + time_embed_dim, hidden_channels)\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        features, time_feat = x[:, :-1], x[:, -1:]\n",
    "        features = F.relu(self.gcn(features, edge_index))\n",
    "        time_embed = self.time_encoder(time_feat)\n",
    "        combined = torch.cat([features, time_embed], dim=1)\n",
    "        return self.dropout(self.feature_fusion(combined))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eef6915-a855-4ce4-ad5f-a33668d1209d",
   "metadata": {},
   "source": [
    "Episodic Graph Pattern Memory preserves historical context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622d8591-c3c5-4a4a-8dde-f0f95169c1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EGPM(torch.nn.Module): \n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.gru = torch.nn.GRU(hidden_channels, hidden_channels, batch_first=True)\n",
    "\n",
    "    def forward(self, x, hidden_state=None):\n",
    "        # x: [n_nodes, hidden_channels]\n",
    "        x, hidden_state = self.gru(x.unsqueeze(0), hidden_state)\n",
    "        return x.squeeze(0), hidden_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7647001d-7ac7-4243-9ac6-758c3d81e0da",
   "metadata": {},
   "source": [
    "It is for faster adaptation to dynamic data distributions and unforeseen concept drifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011be5a7-d7ac-4ce6-b3a7-5de6a550fe5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AdaptiveMetaLearning(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(hidden_channels, hidden_channels)\n",
    "        self.norm = torch.nn.LayerNorm(hidden_channels)\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(F.relu(self.norm(self.linear(x))))\n",
    "\n",
    "\n",
    "def meta_update(model, inner_params, meta_lr=0.001):\n",
    "    with torch.no_grad():\n",
    "        for (n, p), (n_i, p_i) in zip(model.named_parameters(), inner_params):\n",
    "            if p.grad is not None:\n",
    "                p.data = p.data - meta_lr * (p.data - p_i.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31d2a18-2ca3-4ae7-b2c5-a47df359fdf1",
   "metadata": {},
   "source": [
    "MAIN MODEL STRUCTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b9e1bb-5af6-46c1-aad3-b2df9d9e4b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CAST_GRN(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        hidden_channels,\n",
    "        num_classes,\n",
    "        time_embed_dim=32,\n",
    "        original_edge_dim=0,\n",
    "        memory_module: TGNMemory = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.time_norm = torch.nn.LayerNorm(1)\n",
    "        self.embedding1 = TemporalEmbeddingLayer(in_channels, hidden_channels, time_embed_dim)\n",
    "        self.embedding2 = TemporalEmbeddingLayer(hidden_channels, hidden_channels, time_embed_dim)\n",
    "        self.egpm = EGPM(hidden_channels)\n",
    "        self.attn = STSelfAttention(\n",
    "            hidden_channels,\n",
    "            time_embed_dim,\n",
    "            original_edge_dim=original_edge_dim\n",
    "        )\n",
    "        self.meta = AdaptiveMetaLearning(hidden_channels)\n",
    "        self.classifier = torch.nn.Linear(hidden_channels, num_classes)\n",
    "\n",
    "        self.memory_module = memory_module  #External memory_module reference\n",
    "        self.hidden_channels = hidden_channels\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr=None, hidden_state=None):\n",
    "        \n",
    "        time_feat = x[:, -1:].clone()  # Time normalization and embedding\n",
    "        x = torch.cat([x[:, :-1], self.time_norm(time_feat)], dim=1)\n",
    "        x = self.embedding1(x, edge_index)\n",
    "        x = self.embedding2(x, edge_index)\n",
    "\n",
    "        x, hidden_state = self.egpm(x, hidden_state) # Temporal sorting with EGPM (GRU)\n",
    "\n",
    "        if self.memory_module is not None: #TGNMemeory usage step\n",
    "            n_nodes = x.size(0)\n",
    "            mem_list = []\n",
    "            for node_id in range(n_nodes):\n",
    "                mem_vec = self.memory_module.get_memory(node_id)  # CPU’da\n",
    "                mem_list.append(mem_vec)\n",
    "            mem_tensor = torch.stack(mem_list, dim=0).to(x.device)\n",
    "            x = x + mem_tensor\n",
    "\n",
    "        # Self-Attention layer (edge + time embedding)\n",
    "        x = self.attn(x, edge_index, edge_attr=edge_attr, timestamps=time_feat.squeeze())\n",
    "        x = self.meta(x)\n",
    "\n",
    "        out = self.classifier(x)\n",
    "        return out, hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32893135-20e8-4c8b-a8c4-76e7d9d8862e",
   "metadata": {},
   "source": [
    "For indicating the importance of parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873ee15d-313d-45e9-8521-4b790db25e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_fisher_information(model, val_chunks, optimizer, criterion):\n",
    "    fisher = {n: torch.zeros_like(p) for n, p in model.named_parameters()}\n",
    "    model.train()\n",
    "    for chunk in val_chunks:\n",
    "        optimizer.zero_grad()\n",
    "        logits, _ = model(chunk.x, chunk.edge_index, edge_attr=chunk.edge_attr)\n",
    "        loss = criterion(logits, chunk.y)\n",
    "        loss.backward()\n",
    "        for n, p in model.named_parameters():\n",
    "            if p.grad is not None:\n",
    "                fisher[n] += p.grad.pow(2)\n",
    "    for n in fisher:\n",
    "        fisher[n] /= len(val_chunks)\n",
    "    return fisher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccecd347-8328-42f9-9e82-8f88d94ce8da",
   "metadata": {},
   "source": [
    "EWC restricts significant changes to model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce29620-5eeb-46f0-b649-464bb81bcb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ewc_loss(model, fisher, prev_params, lambda_ewc=1.0):\n",
    "    return lambda_ewc * sum(\n",
    "        (fisher[n] * (p - prev_params[n]).pow(2)).sum()\n",
    "        for n, p in model.named_parameters()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a6f099-f351-4337-95c5-7b73d51df44e",
   "metadata": {},
   "source": [
    "To observe class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb6a646-876d-43f2-a8d4-3fa3a872fe0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_label_statistics(chunks, split_name=\"train\"):\n",
    "    print(f\"\\n Label statistics for {split_name} chunks:\")\n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        counts = defaultdict(int)\n",
    "        for label in chunk.y.tolist():\n",
    "            counts[label] += 1\n",
    "        stats = ', '.join(f\"{label}: {counts[label]}\" for label in sorted(counts))\n",
    "        print(f\"  Chunk {i}: {stats}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ad00c5-d6f6-46ea-9bf2-10edc147e47f",
   "metadata": {},
   "source": [
    "Initialize model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e9c405-a149-4316-aaad-d6d43cb369ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initialize_weights(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (torch.nn.Linear, torch.nn.Conv1d, torch.nn.Conv2d)):\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                torch.nn.init.zeros_(m.bias)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d79beda-5d0d-4e7c-9891-6567497abeb9",
   "metadata": {},
   "source": [
    "Definition of plot functions to observe performance of model through different metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa77b456-cdfe-4f71-a54c-3c08977fa256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_progress(epoch_accuracies, epoch_f1_scores,\n",
    "                           epoch_forget_rates, epoch_times, window_forgets,\n",
    "                           all_epoch_acc_dist):\n",
    "    epochs = list(range(1, len(epoch_accuracies) + 1))\n",
    "    plt.figure(figsize=(18, 10))\n",
    "\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.plot(epochs, epoch_accuracies, marker='o')\n",
    "    plt.title('Accuracy over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(2, 3, 2)\n",
    "    plt.plot(epochs, epoch_f1_scores, marker='s', color='orange')\n",
    "    plt.title('F1 Score over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(2, 3, 3)\n",
    "    plt.plot(epochs, epoch_forget_rates, marker='^', color='red')\n",
    "    plt.title('Average Forgetting over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Forgetting')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(2, 3, 4)\n",
    "    plt.plot(epochs, epoch_times, marker='d', color='green')\n",
    "    plt.title('Training Time per Epoch (s)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Seconds')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(2, 3, 5)\n",
    "    plt.plot(epochs, window_forgets, marker='*', color='purple')\n",
    "    plt.title('Windowed Forgetting Rate')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Window Forget')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(2, 3, 6)\n",
    "    sns.boxplot(data=all_epoch_acc_dist)\n",
    "    plt.title('Chunk Accuracy Distribution per Epoch')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xticks(ticks=range(len(all_epoch_acc_dist)), \n",
    "               labels=range(1, len(all_epoch_acc_dist) + 1))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_temporal_coherence_trend(coherence_scores):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(coherence_scores, marker='o')\n",
    "    plt.title(\"Temporal Coherence per Test Chunk\")\n",
    "    plt.xlabel(\"Test Chunk\")\n",
    "    plt.ylabel(\"Coherence Score\")\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_forgetting_curve(cl_metrics):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(cl_metrics.best_acc_per_chunk, label=\"Best Accuracy per Chunk\",\n",
    "             linewidth=2, color='royalblue')\n",
    "    plt.plot(cl_metrics.last_acc_per_chunk, label=\"Last Accuracy per Chunk\",\n",
    "             linewidth=2, color='orange')\n",
    "    plt.title(\"Forgetting Curve (Chunk Accuracy)\")\n",
    "    plt.xlabel(\"Chunk\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff431a9f-3720-4e75-8a6d-0f6c6c02dab5",
   "metadata": {},
   "source": [
    "To achieve optimal balance between rapid adaptation and long-term knowledge retention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb3514e-21fa-48fb-86c0-7e4886d146d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knowledge_distillation_loss(student_logits, teacher_logits, alpha=0.3, temperature=1.0):\n",
    "    kd = F.kl_div(\n",
    "        F.log_softmax(student_logits / temperature, dim=1),\n",
    "        F.softmax(teacher_logits / temperature, dim=1),\n",
    "        reduction='batchmean'\n",
    "    ) * (temperature ** 2)\n",
    "    return alpha * kd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f31cb5-583b-46d2-b336-549321e9e77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(\n",
    "    chunks,\n",
    "    val_chunks,\n",
    "    model,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    buffer,\n",
    "    fisher,\n",
    "    prev_params,\n",
    "    num_epochs=10,\n",
    "    use_meta_update=False,\n",
    "    meta_lr=0.001,\n",
    "    use_tgn_memory=False,\n",
    "    use_kd=False,\n",
    "    kd_alpha=0.3,\n",
    "    kd_temperature=1.0\n",
    "):\n",
    "    if SCHEDULER == \"step\":\n",
    "        scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "        def scheduler_step(val_f1):\n",
    "            scheduler.step()\n",
    "    else:\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode=\"max\", factor=0.5, patience=2)\n",
    "        def scheduler_step(val_f1):\n",
    "            scheduler.step(val_f1)\n",
    "\n",
    "\n",
    "    accs, f1s, forgets, times = [], [], [], []\n",
    "    val_AC, val_F = [], []\n",
    "    window_forgets = []\n",
    "    epoch_chunk_accuracies = []\n",
    "    per_class_f1_history = defaultdict(lambda: [])\n",
    "    replay_class_counter = defaultdict(int)\n",
    "    best_val_f1 = 0.0\n",
    "    best_model_state = None\n",
    "\n",
    "    num_chunks = len(chunks)\n",
    "    cl_metrics = CLMetrics(num_chunks)\n",
    "\n",
    "    if use_tgn_memory:\n",
    "        for chunk in chunks:\n",
    "            n_nodes = chunk.x.size(0)\n",
    "            for nid in range(n_nodes):\n",
    "                model.memory_module.initialize_node(nid)\n",
    "\n",
    "    teacher_model = None\n",
    "    if use_kd:\n",
    "        teacher_model = copy.deepcopy(model)\n",
    "        teacher_model.eval()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_accs = []\n",
    "        epoch_f1s_local = []\n",
    "        start = time.time()\n",
    "\n",
    "        if use_meta_update:\n",
    "            saved_params = [(n, p.clone().detach()) for n, p in model.named_parameters()]\n",
    "\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            optimizer.zero_grad()\n",
    "            logits, hidden_state = model(chunk.x, chunk.edge_index, edge_attr=chunk.edge_attr)\n",
    "            loss = criterion(logits, chunk.y)\n",
    "\n",
    "            # EWC Loss after first epoch\n",
    "            if epoch > 0:\n",
    "                loss += ewc_loss(model, fisher, prev_params)\n",
    "\n",
    "            if epoch > 1 and len(buffer.buffer) > 5: #taking samples from replay\n",
    "                replay_samples = min(5 + epoch, 40)\n",
    "                for replay_data in buffer.sample(replay_samples):\n",
    "                    rlogits, _ = model(replay_data.x, replay_data.edge_index, edge_attr=replay_data.edge_attr)\n",
    "                    loss += criterion(rlogits, replay_data.y)\n",
    "                    for lbl in replay_data.y.tolist():\n",
    "                        replay_class_counter[lbl] += 1\n",
    "\n",
    "            # Knowledge Distillation\n",
    "            if use_kd and teacher_model is not None:\n",
    "                with torch.no_grad():\n",
    "                    tlogits, _ = teacher_model(chunk.x, chunk.edge_index, edge_attr=chunk.edge_attr)\n",
    "                kd_loss_val = knowledge_distillation_loss(\n",
    "                    student_logits=logits,\n",
    "                    teacher_logits=tlogits,\n",
    "                    alpha=kd_alpha,\n",
    "                    temperature=kd_temperature\n",
    "                )\n",
    "                loss += kd_loss_val\n",
    "\n",
    "            loss.backward()\n",
    "            clip_grad_norm_(model.parameters(), 0.8)\n",
    "            optimizer.step()\n",
    "\n",
    "            # update TGNMemory\n",
    "            if use_tgn_memory:\n",
    "                node_ids = torch.arange(chunk.x.size(0))\n",
    "                with torch.no_grad():\n",
    "                    mem_states = model.embedding1(chunk.x, chunk.edge_index)\n",
    "                model.memory_module.update_memory(node_ids, mem_states)\n",
    "\n",
    "            buffer.add(chunk, model)\n",
    "\n",
    "            preds = logits.argmax(dim=1)\n",
    "            acc = accuracy_score(chunk.y.cpu().numpy(), preds.cpu().numpy())\n",
    "            f1_ = f1_score(chunk.y.cpu().numpy(), preds.cpu().numpy(), average='weighted')\n",
    "\n",
    "            # CLMetrics update (best/last accuracy)\n",
    "            cl_metrics.update(i, acc, epoch)\n",
    "\n",
    "            chunk_true = chunk.y.cpu().numpy()\n",
    "            chunk_pred = preds.cpu().numpy()\n",
    "            n_classes = int(chunk.y.max().item()) + 1\n",
    "            prec, rec, f1_scores, _ = precision_recall_fscore_support(\n",
    "                chunk_true, chunk_pred, zero_division=0, labels=range(n_classes)\n",
    "            )\n",
    "            for class_id in range(n_classes):\n",
    "                per_class_f1_history[class_id].append(f1_scores[class_id])\n",
    "\n",
    "            epoch_accs.append(acc)\n",
    "            epoch_f1s_local.append(f1_)\n",
    "\n",
    "            print(f\"Epoch {epoch+1} Chunk {i+1}/{len(chunks)}\"\n",
    "                  f\" | Loss: {loss.item():.4f}\"\n",
    "                  f\" | Acc: {acc:.4f}\"\n",
    "                  f\" | F1: {f1_:.4f}\")\n",
    "\n",
    "        epoch_chunk_accuracies.append(epoch_accs)\n",
    "        pass  # scheduler step moved after validation\n",
    "        times.append(time.time() - start)\n",
    "\n",
    "        avg_acc = np.mean(epoch_accs)\n",
    "        avg_f1 = np.mean(epoch_f1s_local)\n",
    "\n",
    "        current_avg_forget = cl_metrics.average_forgetting()\n",
    "        accs.append(avg_acc)\n",
    "        f1s.append(avg_f1)\n",
    "        forgets.append(current_avg_forget)\n",
    "\n",
    "        # Windowed forgetting\n",
    "        window_vals = []\n",
    "        for idx_ in range(len(epoch_accs)):\n",
    "            if idx_ >= 3:\n",
    "                max_past = max(epoch_accs[idx_-3: idx_])\n",
    "                window_vals.append(max(0, max_past - epoch_accs[idx_]))\n",
    "        avg_window_forget = np.mean(window_vals) if window_vals else 0\n",
    "        window_forgets.append(avg_window_forget)\n",
    "\n",
    "        # Validation\n",
    "        val_accs_epoch = []\n",
    "        val_f1s_epoch = []\n",
    "        model.eval()\n",
    "        for val_chunk in val_chunks:\n",
    "            with torch.no_grad():\n",
    "                val_out, _ = model(val_chunk.x, val_chunk.edge_index, edge_attr=val_chunk.edge_attr)\n",
    "                preds_val = val_out.argmax(dim=1)\n",
    "            vacc = accuracy_score(val_chunk.y.cpu().numpy(), preds_val.cpu().numpy())\n",
    "            vf1 = f1_score(val_chunk.y.cpu().numpy(), preds_val.cpu().numpy(), average='weighted')\n",
    "            val_accs_epoch.append(vacc)\n",
    "            val_f1s_epoch.append(vf1)\n",
    "        val_f1_ = np.mean(val_f1s_epoch)\n",
    "        scheduler_step(val_f1_)\n",
    "        val_acc_ = np.mean(val_accs_epoch)\n",
    "        val_AC.append(val_acc_)\n",
    "        val_F.append(val_f1_)\n",
    "\n",
    "        if use_meta_update:\n",
    "            with torch.no_grad():\n",
    "                new_params = [(n, p.clone().detach()) for n, p in model.named_parameters()]\n",
    "                meta_update(model, new_params, meta_lr=meta_lr)\n",
    "\n",
    "        print(f\"\\n Epoch {epoch+1} Summary:\"\n",
    "              f\" Time: {times[-1]:.2f}s\"\n",
    "              f\" | Avg Train Acc: {avg_acc:.4f}\"\n",
    "              f\" | Avg Train F1: {avg_f1:.4f}\"\n",
    "              f\" | Avg Forgetting: {current_avg_forget:.4f}\"\n",
    "              f\" | Windowed Forget: {avg_window_forget:.4f}\"\n",
    "              f\" | Val Acc: {val_acc_:.4f}\"\n",
    "              f\" | Val F1: {val_f1_:.4f}\\n\")\n",
    "\n",
    "        if use_kd and teacher_model is not None:\n",
    "            teacher_model.load_state_dict(model.state_dict())\n",
    "            teacher_model.eval()\n",
    "\n",
    "        buffer.clean(max_size=300)\n",
    "\n",
    "        if val_f1_ > best_val_f1:\n",
    "            best_val_f1 = val_f1_\n",
    "            best_model_state = model.state_dict()\n",
    "\n",
    "    return (\n",
    "        accs, f1s, forgets, times, best_model_state,\n",
    "        val_AC, val_F, window_forgets,\n",
    "        per_class_f1_history, replay_class_counter,\n",
    "        epoch_chunk_accuracies, cl_metrics\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed1521c-e8ae-43e3-887a-d8400a55003a",
   "metadata": {},
   "source": [
    "To measure prediction consistency across sequential inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b29403-65b6-43f4-aad7-75ce0ed98c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def temporal_coherence(preds, timestamps):\n",
    "    if len(preds) <= 1:\n",
    "        return 1.0\n",
    "    sorted_indices = np.argsort(timestamps)\n",
    "    sorted_preds = np.array(preds)[sorted_indices]\n",
    "    transitions = np.sum(sorted_preds[1:] != sorted_preds[:-1])\n",
    "    max_transitions = len(sorted_preds) - 1\n",
    "    return 1.0 - (transitions / max_transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c29ed4-17ab-4b98-845a-627a46545c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data, verbose=False, log_memory=True):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    data = data.to(device)\n",
    "    with torch.no_grad():\n",
    "        start = time.time()\n",
    "        ram_start = get_memory_usage() if log_memory else None\n",
    "        cuda_mem_start = get_cuda_memory_usage() if log_memory else None\n",
    "\n",
    "        logits, _ = model(data.x, data.edge_index, edge_attr=data.edge_attr)\n",
    "        inf_time = time.time() - start\n",
    "\n",
    "        ram_end = get_memory_usage() if log_memory else None\n",
    "        cuda_mem_end = get_cuda_memory_usage() if log_memory else None\n",
    "\n",
    "        preds = logits.argmax(dim=1).cpu()\n",
    "        acc = accuracy_score(data.y.cpu().numpy(), preds.numpy())\n",
    "        f1 = f1_score(data.y.cpu().numpy(), preds.numpy(), average='weighted')\n",
    "\n",
    "        mem_usage = ram_end if ram_end is not None else None\n",
    "        cuda_usage = cuda_mem_end if cuda_mem_end is not None else None\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Prediction Classes: {torch.unique(preds)}\"\n",
    "                  f\" | True Classes: {torch.unique(data.y)}\")\n",
    "            print(\"Classification Report:\")\n",
    "            print(classification_report(data.y.cpu().numpy(),\n",
    "                                        preds.numpy(), zero_division=0))\n",
    "\n",
    "    return acc, f1, inf_time, preds.numpy(), data.y.cpu().numpy(), mem_usage, cuda_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd21660-8a58-46a3-98ab-9185d860d8eb",
   "metadata": {},
   "source": [
    "Run the integrated main model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b7d433-381c-4379-975b-7f1b9dc379ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    seed_value = 42\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "\n",
    "    data = load_data_from_files()\n",
    "\n",
    "    # Timestamp creation\n",
    "    if os.path.exists(\"timestamps.pt\"):\n",
    "        os.remove(\"timestamps.pt\")\n",
    "\n",
    "    timestamps = generate_balanced_timestamps(data.y)\n",
    "    torch.save(timestamps, \"timestamps.pt\")\n",
    "\n",
    "    timestamps = torch.load(\"timestamps.pt\").float()\n",
    "\n",
    "    # Build chunks -> then split (time-aware)\n",
    "    chunks = shuffle_and_split_data_into_time_chunks(\n",
    "        data, timestamps, chunk_size=CHUNK_SIZE, overlap_ratio=0.1\n",
    "    )\n",
    "    train_chunks, val_chunks, test_chunks = split_chunks_time_aware(\n",
    "        chunks, timestamps, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1\n",
    "    )\n",
    "\n",
    "    device = torch.device(\"cpu\") # you can change with GPU if you have\n",
    "    memory_module = TGNMemory(hidden_dim=128)\n",
    "\n",
    "    original_edge_dim = (\n",
    "        data.edge_attr.size(1)\n",
    "        if hasattr(data, \"edge_attr\") and data.edge_attr is not None\n",
    "        else 0\n",
    "    )\n",
    "\n",
    "    model = CAST_GRN(\n",
    "        in_channels=data.x.size(1),\n",
    "        hidden_channels=128,\n",
    "        num_classes=len(torch.unique(data.y)),\n",
    "        time_embed_dim=32,\n",
    "        original_edge_dim=original_edge_dim,\n",
    "        memory_module=memory_module,\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "    criterion = get_classwise_focal_loss(\n",
    "        num_classes=len(torch.unique(data.y)),\n",
    "        device=data.x.device,\n",
    "    )\n",
    "\n",
    "    buffer = HybridSelectiveReplayBuffer(\n",
    "        max_size=300,\n",
    "        sampling_mode=\"uncertainty-class\",  # chose best sampling mode\n",
    "    )\n",
    "\n",
    "    fisher = compute_fisher_information(model, val_chunks, optimizer, criterion)\n",
    "    prev_params = {n: p.clone() for n, p in model.named_parameters()}\n",
    "\n",
    "    print_label_statistics(train_chunks, \"train\")\n",
    "    print_label_statistics(val_chunks, \"validation\")\n",
    "    print_label_statistics(test_chunks, \"test\")\n",
    "\n",
    "    results = train_model(\n",
    "        chunks=train_chunks,\n",
    "        val_chunks=val_chunks,\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        buffer=buffer,\n",
    "        fisher=fisher,\n",
    "        prev_params=prev_params,\n",
    "        num_epochs=10,\n",
    "        use_meta_update=True,\n",
    "        meta_lr=META_LR,\n",
    "        use_tgn_memory=True,\n",
    "        use_kd=True,\n",
    "        kd_alpha=0.3,\n",
    "        kd_temperature=1.0,\n",
    "    )\n",
    "\n",
    "    (\n",
    "        accs, f1s, forgets, times_list,\n",
    "        best_model_state, val_AC, val_F,\n",
    "        window_forgets, per_class_f1_history,\n",
    "        replay_class_counter, all_epoch_acc_dist,\n",
    "        cl_metrics\n",
    "    ) = results\n",
    "\n",
    "    plot_training_progress(\n",
    "        val_AC, val_F, forgets, times_list, window_forgets, all_epoch_acc_dist\n",
    "    )\n",
    "\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    # Test & analysis\n",
    "    all_preds, all_labels = [], []\n",
    "    test_accs, test_f1s, test_times = [], [], []\n",
    "    coherence_scores = []\n",
    "    test_cpu_mem, test_gpu_mem = [], []\n",
    "\n",
    "    for test_data in test_chunks:\n",
    "        test_data = test_data.to(device)\n",
    "        acc, f1_, t, preds, labels, ram, cuda_mem = evaluate_model(\n",
    "            model, test_data, log_memory=True\n",
    "        )\n",
    "        if len(preds) > 1:\n",
    "            transitions = sum(preds[i] != preds[i + 1] for i in range(len(preds) - 1))\n",
    "            coherence = 1.0 - transitions / (len(preds) - 1)\n",
    "        else:\n",
    "            coherence = 1.0\n",
    "\n",
    "        test_accs.append(acc)\n",
    "        test_f1s.append(f1_)\n",
    "        test_times.append(t)\n",
    "        coherence_scores.append(coherence)\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels)\n",
    "        test_cpu_mem.append(ram)\n",
    "        test_gpu_mem.append(cuda_mem)\n",
    "\n",
    "    print(\"\\n Final Combined Classification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, zero_division=0))\n",
    "\n",
    "    plot_temporal_coherence_trend(coherence_scores)\n",
    "\n",
    "    print(\"\\n=== Final Test Results ===\")\n",
    "    print(f\" Avg Test Accuracy: {np.mean(test_accs):.4f} ± {np.std(test_accs):.4f}\")\n",
    "    print(f\" Avg F1 Score: {np.mean(test_f1s):.4f} ± {np.std(test_f1s):.4f}\")\n",
    "    print(f\" Avg Temporal Coherence: {np.mean(coherence_scores):.4f} ± {np.std(coherence_scores):.4f}\")\n",
    "    print(f\" Avg Inference Time: {np.mean(test_times):.4f}s ± {np.std(test_times):.4f}\")\n",
    "\n",
    "    # Continual metrics\n",
    "    avg_forget = cl_metrics.average_forgetting() * 100\n",
    "    std_forget = cl_metrics.average_forgetting_std() * 100\n",
    "    avg_adapt = cl_metrics.adaptation_speed()\n",
    "    std_adapt = cl_metrics.adaptation_speed_std()\n",
    "\n",
    "    print(\"\\n=== CONTINUAL METRICS ===\")\n",
    "    print(f\"Average Forgetting (%): {avg_forget:.4f} ± {std_forget:.4f}\")\n",
    "    if avg_adapt is not None:\n",
    "        print(f\"Adaptation Speed (epoch of peak acc): {avg_adapt:.4f} ± {std_adapt:.4f}\")\n",
    "    else:\n",
    "        print(\"Adaptation Speed (epoch of peak acc): None\")\n",
    "\n",
    "    lat_ms = np.array(test_times) * 1000.0\n",
    "    avg_lat = np.mean(lat_ms)\n",
    "    std_lat = np.std(lat_ms)\n",
    "    print(f\"Average Inference Latency (ms): {avg_lat:.3f} ± {std_lat:.3f}\")\n",
    "\n",
    "    plot_confusion_matrix(all_labels, all_preds, num_classes=len(torch.unique(data.y)))\n",
    "    plot_forgetting_curve(cl_metrics)\n",
    "    #plot_memory_usage(test_cpu_mem, test_gpu_mem)\n",
    "\n",
    "    drift_flags_acc = detect_concept_drift(test_accs, window=5, threshold=2.5)\n",
    "    print(\"Z-Score drift (Accuracy):\", [i for i, f in enumerate(drift_flags_acc) if f])\n",
    "    plot_drift_detection(test_accs, drift_flags_acc, \"Accuracy\")\n",
    "\n",
    "    drift_flags_adw = detect_drift_adwin(test_accs, delta=0.5)\n",
    "    print(\"ADWIN (river) drift (Accuracy):\", [i for i, f in enumerate(drift_flags_adw) if f])\n",
    "    plot_adwin_drift(test_accs, drift_flags_adw)\n",
    "\n",
    "    drift_flags_ph = detect_drift_pagehinkley(test_accs, threshold=2, alpha=0.9, min_instances=2)\n",
    "    print(\"Page-Hinkley drift (Accuracy):\", [i for i, f in enumerate(drift_flags_ph) if f])\n",
    "\n",
    "    x = np.arange(len(test_accs))\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(x, test_accs, label=\"Chunk Accuracy\")\n",
    "    drift_points = [i for i, flag in enumerate(drift_flags_ph) if flag]\n",
    "    plt.scatter(\n",
    "        drift_points, [test_accs[i] for i in drift_points],\n",
    "        label=\"Page-Hinkley Drift\", zorder=5\n",
    "    )\n",
    "    plt.xlabel(\"Test Chunk\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Page-Hinkley Concept Drift Detection\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157bb51c-4d5f-4c83-8985-5d71a944e29b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
